{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from joblib import dump\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "MODEL_INPUTS_OUTPUTS = os.path.join(ROOT_DIR, 'model_inputs_outputs/')\n",
    "INPUT_DIR = os.path.join(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = os.path.join(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = os.path.join(INPUT_DIR, \"data\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"training\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = os.path.join(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = os.path.join(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = os.path.join(MODEL_ARTIFACTS_PATH, 'ohe.joblib')\n",
    "PREDICTOR_DIR_PATH = os.path.join(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = os.path.join(PREDICTOR_DIR_PATH, \"predictor.joblib\")\n",
    "IMPUTATION_FILE = os.path.join(MODEL_ARTIFACTS_PATH, 'imputation.joblib')\n",
    "LABEL_ENCODER_FILE = os.path.join(MODEL_ARTIFACTS_PATH, 'label_encoder.joblib')\n",
    "if not os.path.exists(MODEL_ARTIFACTS_PATH):\n",
    "    os.makedirs(MODEL_ARTIFACTS_PATH)\n",
    "if not os.path.exists(PREDICTOR_DIR_PATH):\n",
    "    os.makedirs(PREDICTOR_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema\n",
    "The schema contains metadata about the datasets. We will use the scehma to get information about the type of each feature (NUMERIC or CATEGORICAL) and the id and target features, this will be helpful in preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = [f for f in os.listdir(INPUT_SCHEMA_DIR) if f.endswith('json')][0]\n",
    "schema_path = os.path.join(INPUT_SCHEMA_DIR, file_name)\n",
    "with open(schema_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    schema = json.load(file)\n",
    "features = schema['features']\n",
    "\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "for f in features:\n",
    "    if f['dataType'] == 'CATEGORICAL':\n",
    "        categorical_features.append(f['name'])\n",
    "    else:\n",
    "        numeric_features.append(f['name'])\n",
    "\n",
    "id_feature = schema['id']['name']\n",
    "target_feature = schema['target']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>color</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YDDKTO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.7411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FPLK2Z</td>\n",
       "      <td>3.3782</td>\n",
       "      <td>Red</td>\n",
       "      <td>26.2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P2YCAP</td>\n",
       "      <td>2.2480</td>\n",
       "      <td>Blue</td>\n",
       "      <td>112.3547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMT8XP</td>\n",
       "      <td>1.9806</td>\n",
       "      <td>Blue</td>\n",
       "      <td>109.8264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5D9Q5F</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>Red</td>\n",
       "      <td>1.0144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  number color    target\n",
       "0  YDDKTO     NaN   NaN   84.7411\n",
       "1  FPLK2Z  3.3782   Red   26.2021\n",
       "2  P2YCAP  2.2480  Blue  112.3547\n",
       "3  IMT8XP  1.9806  Blue  109.8264\n",
       "4  5D9Q5F  0.5048   Red    1.0144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = [f for f in os.listdir(TRAIN_DIR) if f.endswith('.csv')][0]\n",
    "file_path = os.path.join(TRAIN_DIR, file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is very important before training the model, as the data may contain missing values in some cells. Moreover, most of the learning algorithms cannot work with categorical data, thus the data has to be encoded.\n",
    "\n",
    "In this section we will impute the missing values and encode the categorical features. Afterwards the data will be ready to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing missing data\n",
    "> The median value will be used to impute missing values of the numeric features and the mode will be used to impute categorical features.\n",
    "\n",
    "##### You can add your own preprocessing steps such as:\n",
    "<ul>\n",
    "<li>Normalization</li> <br>\n",
    "<li>Outlier removal</li><br>\n",
    "<li>Handling imbalanced classes</li><br>\n",
    "<li>Dropping or adding features</li><br>\n",
    "</ul>\n",
    "\n",
    "### Important note:\n",
    "<p> \n",
    "Saving the values used for imputation during training step is crucial. These values will be used to impute missing data in the testing set. This is very important to avoid the well known problem of data leakage. During testing, you should not make any assumptions about the data in hand, alternatively anything needed during the testing phase should be learned from the training phase. This is why we are creating a dictionary of values used during training to reuse these values during testing.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/moo/Desktop/Upwork/rt-ML/Jupyter-Notebook-Python-Regression-Template/model_inputs_outputs/model/artifacts/imputation.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputing missing data\n",
    "imputation_values = {}\n",
    "columns_with_missing_values = df.columns[df.isna().any()]\n",
    "for column in columns_with_missing_values:    \n",
    "    if column in numeric_features:\n",
    "        value = df[column].median()\n",
    "    else:\n",
    "        value = df[column].mode()[0]\n",
    "    df[column].fillna(value, inplace=True)\n",
    "    imputation_values[column] = value\n",
    "\n",
    "dump(imputation_values, IMPUTATION_FILE)\n",
    "\n",
    "# Comment the above code and write you own imputation code here\n",
    "\n",
    "\n",
    "#BEGIN\n",
    "\n",
    "#CODE HERE\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Categorical features\n",
    "<p>\n",
    "The id column is just an identifier for the training example, so we will exclude it during the encoding phase.<br>\n",
    "Target feature will be label encoded in the next step.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the id and target columns in a different variable.\n",
    "ids = df[id_feature]\n",
    "target = df[target_feature]\n",
    "\n",
    "# Dropping the id and target from the dataframe\n",
    "df.drop(columns=[id_feature, target_feature], inplace=True)\n",
    "\n",
    "# Ensure that all categorical columns are stored as str type.\n",
    "# This is to ensure that even if the categories are numbers (1, 2, ...), they still get encoded.\n",
    "for c in categorical_features:\n",
    "    df[c] = df[c].astype(str)\n",
    "\n",
    "# Encoding the categorical features if exist\n",
    "if categorical_features:\n",
    "    encoder = OneHotEncoder(top_categories=6)\n",
    "    encoder.fit(df)\n",
    "    df = encoder.transform(df)\n",
    "\n",
    "    # Saving the encoder to use it on the testing dataset\n",
    "    path = dump(encoder, OHE_ENCODER_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "We choose Linear Regression model, but feel free to try your own and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a linear regression model and training it\n",
    "model = LinearRegression()\n",
    "model.fit(df, target)\n",
    "\n",
    "# BEGIN\n",
    "\n",
    "# model = ...\n",
    "\n",
    "# END\n",
    "\n",
    "# Saving the model to use it for predictions\n",
    "path = dump(model, PREDICTOR_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
